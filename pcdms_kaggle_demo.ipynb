{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Initialize Notebook","metadata":{}},{"cell_type":"markdown","source":"## ðŸ“– Citation\n\nThis notebook is a replication of the [PCDM demo](https://github.com/tencent-ailab/PCDMs/blob/main/pcdms_demo.ipynb) and is based on the following paper:\n\n> Shen, F., Ye, H., Zhang, J., Wang, C., Han, X., & Wei, Y.  \n> *Advancing Pose-Guided Image Synthesis with Progressive Conditional Diffusion Models*.  \n> The Twelfth International Conference on Learning Representations (ICLR).\n\n```bibtex\n@inproceedings{shenadvancing,\n  title={Advancing Pose-Guided Image Synthesis with Progressive Conditional Diffusion Models},\n  author={Shen, Fei and Ye, Hu and Zhang, Jun and Wang, Cong and Han, Xiao and Wei, Yang},\n  booktitle={The Twelfth International Conference on Learning Representations}\n}\n","metadata":{}},{"cell_type":"code","source":"!pip install open_clip_torch controlnet-aux mediapipe > /dev/null\n!pip install -U diffusers > /dev/null","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:48:43.415011Z","iopub.execute_input":"2025-02-21T12:48:43.415301Z","iopub.status.idle":"2025-02-21T12:49:08.266651Z","shell.execute_reply.started":"2025-02-21T12:48:43.415276Z","shell.execute_reply":"2025-02-21T12:49:08.265577Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import basics\nfrom pathlib import Path\nimport numpy as np\nfrom PIL import Image\nfrom tqdm.auto import tqdm\nfrom matplotlib import pyplot as plt\n# basics pytorch\nimport torch\nimport torch.nn as nn\nimport torchvision\nimport torch.nn.functional as F\nfrom torchvision import transforms\n# import image encoder\nfrom transformers import CLIPImageProcessor, Dinov2Model\n# import diffusion models\nfrom diffusers import (\n    AutoencoderKL,           # Autoencoder model\n    DDIMScheduler,           # Scheduler for diffusion steps\n    UNet2DConditionModel     # Conditional U-Net model\n)\nfrom diffusers.models.controlnets.controlnet import ControlNetConditioningEmbedding\n\n# constants uses through whole codebase\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ngenerator = torch.Generator(device=device).manual_seed(42)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:49:08.267637Z","iopub.execute_input":"2025-02-21T12:49:08.267870Z","iopub.status.idle":"2025-02-21T12:49:28.662977Z","shell.execute_reply.started":"2025-02-21T12:49:08.267851Z","shell.execute_reply":"2025-02-21T12:49:28.662320Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Clone Repo","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/tencent-ailab/PCDMs.git\n!mv PCDMs/* .\n!ls","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:49:28.664258Z","iopub.execute_input":"2025-02-21T12:49:28.664816Z","iopub.status.idle":"2025-02-21T12:49:32.046844Z","shell.execute_reply.started":"2025-02-21T12:49:28.664793Z","shell.execute_reply":"2025-02-21T12:49:32.045940Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# fix some importing\n# replace 'diffusers.models.unets.unet_2d_blocks' with 'diffusers.models.unet_2d_blocks' in 'stage2_inpaint_unet_2d_condition.py'\n!sed -i 's/diffusers\\.models\\.unet_2d_blocks/diffusers.models.unets.unet_2d_blocks/g' ./src/models/stage2_inpaint_unet_2d_condition.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:49:32.048557Z","iopub.execute_input":"2025-02-21T12:49:32.048899Z","iopub.status.idle":"2025-02-21T12:49:32.184908Z","shell.execute_reply.started":"2025-02-21T12:49:32.048867Z","shell.execute_reply":"2025-02-21T12:49:32.183970Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## DWpose\n\nI will use `easy-dwpose` instead of `mmpose` which more lightweight because they use onnx\n\n**DON'T** move download cell upward will cause error","metadata":{}},{"cell_type":"code","source":"!pip install easy-dwpose > /dev/null","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:49:32.186106Z","iopub.execute_input":"2025-02-21T12:49:32.186447Z","iopub.status.idle":"2025-02-21T12:49:48.262892Z","shell.execute_reply.started":"2025-02-21T12:49:32.186415Z","shell.execute_reply":"2025-02-21T12:49:48.261987Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# replace single_extract_pose.py\nfrom easy_dwpose  import DWposeDetector\n\ndef inference_pose(img_path, image_size=(1024, 1024)):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    model = DWposeDetector(device=device)\n    pil_image = Image.open(img_path).convert(\"RGB\").resize(image_size, Image.BICUBIC)\n    dwpose_image = model(pil_image, output_type='np', detect_resolution=image_size[1])\n    save_dwpose_image = Image.fromarray(dwpose_image)\n    return save_dwpose_image\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:49:48.264021Z","iopub.execute_input":"2025-02-21T12:49:48.264407Z","iopub.status.idle":"2025-02-21T12:49:48.548679Z","shell.execute_reply.started":"2025-02-21T12:49:48.264372Z","shell.execute_reply":"2025-02-21T12:49:48.548022Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Download Checkpoint","metadata":{}},{"cell_type":"code","source":"! gdown \"1JFFy_FBxOFuGFBcB6xMIVwcQb8bfnpO9\" -O \"pcdms_ckpt.pt\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:49:48.549457Z","iopub.execute_input":"2025-02-21T12:49:48.549761Z","iopub.status.idle":"2025-02-21T12:50:22.415494Z","shell.execute_reply.started":"2025-02-21T12:49:48.549731Z","shell.execute_reply":"2025-02-21T12:50:22.414651Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Load Checkpoint","metadata":{}},{"cell_type":"code","source":"def load_model(ckpt_path = \"./pcdms_ckpt.pt\"):\n    model_ckpt = torch.load(ckpt_path)\n    \n    unet_dict = {}\n    pose_proj_dict = {}\n    image_proj_model_dict = {}\n\n    for key, value in model_ckpt['module'].items():\n        # sub models\n        model_name = key.split('.')[0]\n        model_key = key[len(model_name)+1:]\n        # put weights in correct dict\n        if model_name == 'pose_proj':\n            pose_proj_dict[model_key] = value\n        elif model_name == 'unet':\n            unet_dict[model_key] = value\n        elif model_name == 'image_proj_model':\n            image_proj_model_dict[model_key] = value\n        else:\n            raise FileNotFoundError(\"no model called that\")\n        \n    return unet_dict,  pose_proj_dict,  image_proj_model_dict\n\n\nunet_dict,  pose_proj_dict,  image_proj_model_dict = load_model()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:50:22.418404Z","iopub.execute_input":"2025-02-21T12:50:22.418652Z","iopub.status.idle":"2025-02-21T12:50:23.568410Z","shell.execute_reply.started":"2025-02-21T12:50:22.418631Z","shell.execute_reply":"2025-02-21T12:50:23.567697Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load Components","metadata":{}},{"cell_type":"code","source":"# loading sd components\n\nfrom src.models.stage2_inpaint_unet_2d_condition import Stage2_InapintUNet2DConditionModel\n\n# load unet from \"stable diffusion v2.1\" and fed it to stag2model\nunet = Stage2_InapintUNet2DConditionModel.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\", torch_dtype=torch.float16,subfolder=\"unet\",in_channels=9, low_cpu_mem_usage=False, ignore_mismatched_sizes=True).to(device)\n\n# load vae I didn't see it in use in this notebook\n# vae = AutoencoderKL.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\",subfolder=\"vae\").to(device, dtype=torch.float16)\nnoise_scheduler = DDIMScheduler(\n    num_train_timesteps=1000,\n    beta_start=0.00085,\n    beta_end=0.012,\n    beta_schedule=\"scaled_linear\",\n    clip_sample=False,\n    set_alpha_to_one=False,\n    steps_offset=1,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:50:23.569647Z","iopub.execute_input":"2025-02-21T12:50:23.569957Z","iopub.status.idle":"2025-02-21T12:50:43.244950Z","shell.execute_reply.started":"2025-02-21T12:50:23.569921Z","shell.execute_reply":"2025-02-21T12:50:43.243981Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# image encoder\nimage_encoder = Dinov2Model.from_pretrained(\"facebook/dinov2-giant\").to(device, dtype=torch.float16)\n\n\n# ImageProjModel will project `embeddings` output from `image_encoder` to input to SD\nclass ImageProjModel(torch.nn.Module):\n    \"\"\"SD model with image prompt\"\"\"\n    def __init__(self, in_dim, hidden_dim, out_dim, dropout = 0.):\n        super().__init__()\n\n        self.net = nn.Sequential(\n            nn.Linear(in_dim, hidden_dim),\n            nn.GELU(),\n            nn.Dropout(dropout),\n            nn.LayerNorm(hidden_dim),\n            nn.Linear(hidden_dim, out_dim),\n            nn.Dropout(dropout)\n        )\n\n    def forward(self, x):  \n        return self.net(x)\nimage_proj_model = ImageProjModel(in_dim=1536, hidden_dim=768, out_dim=1024).to(device).to(dtype=torch.float16)\n\n# pose encoder\npose_proj_model = ControlNetConditioningEmbedding(\n    conditioning_embedding_channels=320,\n    block_out_channels=(16, 32, 96, 256),\n    conditioning_channels=3).to(device).to(dtype=torch.float16)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:50:43.245937Z","iopub.execute_input":"2025-02-21T12:50:43.246369Z","iopub.status.idle":"2025-02-21T12:52:13.599958Z","shell.execute_reply.started":"2025-02-21T12:50:43.246325Z","shell.execute_reply":"2025-02-21T12:52:13.599199Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from src.pipelines.PCDMs_pipeline import PCDMsPipeline\n\n# pipeline of stage2\npipe = PCDMsPipeline.from_pretrained(\"stabilityai/stable-diffusion-2-1-base\", unet=unet,  torch_dtype=torch.float16, scheduler=noise_scheduler,feature_extractor=None,safety_checker=None).to(device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:52:13.600895Z","iopub.execute_input":"2025-02-21T12:52:13.601234Z","iopub.status.idle":"2025-02-21T12:52:21.569856Z","shell.execute_reply.started":"2025-02-21T12:52:13.601199Z","shell.execute_reply":"2025-02-21T12:52:21.569194Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Load checkpoint weights","metadata":{}},{"cell_type":"code","source":"unet.load_state_dict(unet_dict)\npose_proj_model.load_state_dict(pose_proj_dict)\nimage_proj_model.load_state_dict(image_proj_model_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:52:21.570563Z","iopub.execute_input":"2025-02-21T12:52:21.570798Z","iopub.status.idle":"2025-02-21T12:52:21.994209Z","shell.execute_reply.started":"2025-02-21T12:52:21.570778Z","shell.execute_reply":"2025-02-21T12:52:21.993463Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference Step By Step","metadata":{}},{"cell_type":"code","source":"# transformer of data\nclip_image_processor = CLIPImageProcessor()\nimg_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize([0.5], [0.5]), # transform pixels in range [-1, 1]\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:52:21.995001Z","iopub.execute_input":"2025-02-21T12:52:21.995296Z","iopub.status.idle":"2025-02-21T12:52:21.999071Z","shell.execute_reply.started":"2025-02-21T12:52:21.995262Z","shell.execute_reply":"2025-02-21T12:52:21.998219Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"will load image sample 1 step by step","metadata":{}},{"cell_type":"code","source":"# define some parameters\nnum_samples = 1\nimage_size = (512, 512)\ns_img_path = './imgs/img1.png'\ntarget_pose_img = './imgs/pose1.png'","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:52:21.999990Z","iopub.execute_input":"2025-02-21T12:52:22.000294Z","iopub.status.idle":"2025-02-21T12:52:22.014388Z","shell.execute_reply.started":"2025-02-21T12:52:22.000262Z","shell.execute_reply":"2025-02-21T12:52:22.013666Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Preparing Inputs\n- In painting input (image || black) + mask of (white || black)\n- Pose Condition (src pose || target pose)\n- ","metadata":{}},{"cell_type":"markdown","source":"### In-painting input","metadata":{}},{"cell_type":"code","source":"s_img = Image.open(s_img_path).convert(\"RGB\").resize(image_size, Image.BICUBIC)\nblack_image = Image.new(\"RGB\", s_img.size, (0, 0, 0)).resize(image_size, Image.BICUBIC)\n\ns_img_t_mask = Image.new(\"RGB\", (s_img.width * 2, s_img.height))\ns_img_t_mask.paste(s_img, (0, 0))\ns_img_t_mask.paste(black_image, (s_img.width, 0))\n\nvae_image = torch.unsqueeze(img_transform(s_img_t_mask), 0)\nprint(\"s_img_t_mask (vae input) image shape: \", vae_image.shape)\n\ns_img_t_mask","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:52:22.015048Z","iopub.execute_input":"2025-02-21T12:52:22.015247Z","iopub.status.idle":"2025-02-21T12:52:22.165945Z","shell.execute_reply.started":"2025-02-21T12:52:22.015230Z","shell.execute_reply":"2025-02-21T12:52:22.165067Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Mask (In-Painting input)","metadata":{}},{"cell_type":"code","source":"mask1 = torch.ones((1, 1, int(image_size[0] / 8), int(image_size[1] / 8))).to(device, dtype=torch.float16)\nmask0 = torch.zeros((1, 1, int(image_size[0] / 8), int(image_size[1] / 8))).to(device, dtype=torch.float16)\nmask = torch.cat([mask1, mask0], dim=3)\n\nprint(\"mask shape: \", mask.shape)\nplt.imshow(mask[0].detach().cpu().permute(1, 2, 0), cmap=\"gray\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:52:22.166990Z","iopub.execute_input":"2025-02-21T12:52:22.167264Z","iopub.status.idle":"2025-02-21T12:52:22.473913Z","shell.execute_reply.started":"2025-02-21T12:52:22.167240Z","shell.execute_reply":"2025-02-21T12:52:22.472683Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Pose Condition","metadata":{}},{"cell_type":"markdown","source":"put `dwpose` of source image beside `openpose` of target","metadata":{}},{"cell_type":"code","source":"s_pose = inference_pose(s_img_path, image_size=(image_size[1], image_size[0])).resize(image_size, Image.BICUBIC)\nprint('source image width: {}, height: {}'.format(s_pose.width, s_pose.height))\n\nt_pose = Image.open(target_pose_img).convert(\"RGB\").resize((image_size), Image.BICUBIC)\n\nst_pose = Image.new(\"RGB\", (s_pose.width * 2, s_pose.height))\nst_pose.paste(s_pose, (0, 0))\nst_pose.paste(t_pose, (s_pose.width, 0))\n\ncond_st_pose = torch.unsqueeze(img_transform(st_pose), 0)\nprint(\"st_pose (condition) shape: \", cond_st_pose.shape)\nst_pose","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:52:22.474904Z","iopub.execute_input":"2025-02-21T12:52:22.475277Z","iopub.status.idle":"2025-02-21T12:52:34.776001Z","shell.execute_reply.started":"2025-02-21T12:52:22.475241Z","shell.execute_reply":"2025-02-21T12:52:34.775173Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Clip image encoder processor  -> input to Dino :XD","metadata":{}},{"cell_type":"code","source":"# ??clip_image_processor","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:52:34.776732Z","iopub.execute_input":"2025-02-21T12:52:34.776975Z","iopub.status.idle":"2025-02-21T12:52:34.780473Z","shell.execute_reply.started":"2025-02-21T12:52:34.776947Z","shell.execute_reply":"2025-02-21T12:52:34.779523Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clip_s_img = clip_image_processor(images=s_img, return_tensors=\"pt\").pixel_values # [1, 3, 224, 224]\nprint(\"clip shape: \", clip_s_img.shape)\nplt.imshow(clip_s_img[0].permute(1, 2, 0))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:52:34.781352Z","iopub.execute_input":"2025-02-21T12:52:34.781656Z","iopub.status.idle":"2025-02-21T12:52:35.001609Z","shell.execute_reply.started":"2025-02-21T12:52:34.781622Z","shell.execute_reply":"2025-02-21T12:52:35.000796Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Inference","metadata":{}},{"cell_type":"code","source":"# just to make sure I don't have replication of models on GPU\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:52:35.002565Z","iopub.execute_input":"2025-02-21T12:52:35.002820Z","iopub.status.idle":"2025-02-21T12:52:35.006300Z","shell.execute_reply.started":"2025-02-21T12:52:35.002798Z","shell.execute_reply":"2025-02-21T12:52:35.005534Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# preprocessing step -- prepare latents, encoded embeddings, conditions\n\nwith torch.inference_mode():\n    # 1. prepare conditioned pose\n    cond_pose = pose_proj_model(cond_st_pose.to(dtype=torch.float16, device=device))\n    # 2. prepare latent \n    simg_mask_latents = pipe.vae.encode(vae_image.to(device, dtype=torch.float16)).latent_dist.sample()\n    simg_mask_latents = simg_mask_latents * 0.18215 # since VAE paper do that \n    # projected encoded embeddings for both (conditional & uncondational)\n    images_embeds = image_encoder(clip_s_img.to(device, dtype=torch.float16)).last_hidden_state\n    image_prompt_embeds = image_proj_model(images_embeds)\n    uncond_image_prompt_embeds = image_proj_model(torch.zeros_like(images_embeds))\n\n\nbs_embed, seq_len, _ = image_prompt_embeds.shape\n# repeat inputs to count for unconditional embeddings\nimage_prompt_embeds = image_prompt_embeds.repeat(1, num_samples, 1)\nimage_prompt_embeds = image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)\nuncond_image_prompt_embeds = uncond_image_prompt_embeds.repeat(1, num_samples, 1)\nuncond_image_prompt_embeds = uncond_image_prompt_embeds.view(bs_embed * num_samples, seq_len, -1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:52:35.007185Z","iopub.execute_input":"2025-02-21T12:52:35.007450Z","iopub.status.idle":"2025-02-21T12:52:35.995643Z","shell.execute_reply.started":"2025-02-21T12:52:35.007428Z","shell.execute_reply":"2025-02-21T12:52:35.994685Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# parameters you could play with\nnum_inference_steps = 50\nguidance_scale = 2.0\nnum_samples = 1\n\noutput = pipe(\n    simg_mask_latents= simg_mask_latents,\n    mask = mask,\n    cond_pose = cond_pose,\n    prompt_embeds=image_prompt_embeds,\n    negative_prompt_embeds=uncond_image_prompt_embeds, # ??\n    height=image_size[1],\n    width=image_size[0]*2, # for inpainting mask\n    num_images_per_prompt=1,\n    guidance_scale=guidance_scale,\n    generator=generator,\n    num_inference_steps=num_inference_steps,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:52:35.996638Z","iopub.execute_input":"2025-02-21T12:52:35.996941Z","iopub.status.idle":"2025-02-21T12:53:04.211857Z","shell.execute_reply.started":"2025-02-21T12:52:35.996915Z","shell.execute_reply":"2025-02-21T12:53:04.210899Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output.images[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:53:04.215142Z","iopub.execute_input":"2025-02-21T12:53:04.215377Z","iopub.status.idle":"2025-02-21T12:53:04.294522Z","shell.execute_reply.started":"2025-02-21T12:53:04.215358Z","shell.execute_reply":"2025-02-21T12:53:04.293601Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Inference in Function","metadata":{}},{"cell_type":"code","source":"def get_inpainting_inputs(s_img):\n    \"\"\" concatenate source with mask - do basic processing - return latents output from VAE \"\"\"\n    # 1. concatenate source with mask\n    black_image = Image.new(\"RGB\", s_img.size, (0, 0, 0)).resize(s_img.size, Image.BICUBIC)\n    \n    s_img_t_mask = Image.new(\"RGB\", (s_img.width * 2, s_img.height))\n    s_img_t_mask.paste(s_img, (0, 0))\n    s_img_t_mask.paste(black_image, (s_img.width, 0))\n    # 2. do basic processing\n    vae_image = torch.unsqueeze(img_transform(s_img_t_mask), 0)\n    # 3. get latents from VAE\n    with torch.inference_mode():\n        latents = pipe.vae.encode(vae_image.to(device, dtype=torch.float16)).latent_dist.sample()\n        latents = latents * 0.18215 # since VAE paper do that \n        \n    return latents\n\ndef get_inpainting_cond(s_img, t_pose):\n    \"\"\" concatenate source pose with target pose -- project conditions\"\"\"\n    # 1. concatenate source pose with target pose\n    s_pose = inference_pose(s_img_path, image_size=s_img.size).resize(s_img.size, Image.BICUBIC)\n    st_pose = Image.new(\"RGB\", (s_pose.width * 2, s_pose.height))\n    st_pose.paste(s_pose, (0, 0))\n    st_pose.paste(t_pose, (s_pose.width, 0))\n\n    # 2. project conditions\n    cond_st_pose = torch.unsqueeze(img_transform(st_pose), 0)\n    with torch.inference_mode():\n        cond_pose = pose_proj_model(cond_st_pose.to(dtype=torch.float16, device=device))\n    \n    return cond_pose\n\ndef get_image_embeddings(s_img, num_samples = 1):\n    # do basic processing \n    clip_s_img = clip_image_processor(images=s_img, return_tensors=\"pt\").pixel_values \n    # projected encoded embeddings for both (conditional & uncondational)\n    with torch.inference_mode():\n        images_embeds = image_encoder(clip_s_img.to(device, dtype=torch.float16)).last_hidden_state\n        image_prompt_embeds = image_proj_model(images_embeds)\n        uncond_image_prompt_embeds = image_proj_model(torch.zeros_like(images_embeds))\n    \n    # repeat inputs to count for unconditional embeddings\n    bs_embed, seq_len, _ = image_prompt_embeds.shape\n    image_prompt_embeds = image_prompt_embeds.repeat(1, num_samples, 1).view(bs_embed * num_samples, seq_len, -1)\n    uncond_image_prompt_embeds = uncond_image_prompt_embeds.repeat(1, num_samples, 1).view(bs_embed * num_samples, seq_len, -1)\n    \n    return image_prompt_embeds, uncond_image_prompt_embeds","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:53:04.296360Z","iopub.execute_input":"2025-02-21T12:53:04.296692Z","iopub.status.idle":"2025-02-21T12:53:04.308251Z","shell.execute_reply.started":"2025-02-21T12:53:04.296665Z","shell.execute_reply":"2025-02-21T12:53:04.307235Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def inference_one_image(s_img_path = './imgs/img1.png', target_pose_path = './imgs/pose1.png', \n                        image_size = (512, 512),\n                        num_inference_steps = 50,\n                        guidance_scale = 2.0):\n    # ======================== Preprocessing ==========================================\n    # 1. read image\n    s_img = Image.open(s_img_path).convert(\"RGB\").resize(image_size, Image.BICUBIC)\n    t_pose = Image.open(target_pose_path).convert(\"RGB\").resize((image_size), Image.BICUBIC)\n    # 2. get inpainting input\n    simg_mask_latents = get_inpainting_inputs(s_img)\n    # 3. get conditional pose\n    cond_pose = get_inpainting_cond(s_img, t_pose)\n    # 4. get image embeddings\n    image_prompt_embeds, uncond_image_prompt_embeds = get_image_embeddings(s_img)\n\n    # ======================== Pipeline  ==========================================\n    return pipe(\n            simg_mask_latents= simg_mask_latents,\n            mask = mask,\n            cond_pose = cond_pose,\n            prompt_embeds=image_prompt_embeds,\n            negative_prompt_embeds=uncond_image_prompt_embeds, # ??\n            height=image_size[1],\n            width=image_size[0]*2, # for inpainting mask\n            num_images_per_prompt=1,\n            guidance_scale=guidance_scale,\n            generator=generator,\n            num_inference_steps=num_inference_steps,\n        )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:53:04.309237Z","iopub.execute_input":"2025-02-21T12:53:04.309521Z","iopub.status.idle":"2025-02-21T12:53:04.327569Z","shell.execute_reply.started":"2025-02-21T12:53:04.309494Z","shell.execute_reply":"2025-02-21T12:53:04.326853Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Outputs\n\n- it's clear that model change src image and not perserve it (IDK it's about inpainting or not)","metadata":{}},{"cell_type":"code","source":"inference_one_image().images[-1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:53:04.328327Z","iopub.execute_input":"2025-02-21T12:53:04.328586Z","iopub.status.idle":"2025-02-21T12:53:35.078016Z","shell.execute_reply.started":"2025-02-21T12:53:04.328555Z","shell.execute_reply":"2025-02-21T12:53:35.077012Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"s_img_path = './imgs/img2.png'\ntarget_pose_path = './imgs/pose1.png'\noutput = inference_one_image(s_img_path, target_pose_path).images[-1]\n\noutput.resize((512, 512), Image.BICUBIC)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:53:35.078922Z","iopub.execute_input":"2025-02-21T12:53:35.079155Z","iopub.status.idle":"2025-02-21T12:54:05.536298Z","shell.execute_reply.started":"2025-02-21T12:53:35.079135Z","shell.execute_reply":"2025-02-21T12:54:05.535344Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output = inference_one_image('./imgs/img3.png', target_pose_path).images[-1]\noutput.resize((512, 512), Image.BICUBIC)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:54:05.537344Z","iopub.execute_input":"2025-02-21T12:54:05.537571Z","iopub.status.idle":"2025-02-21T12:54:36.047101Z","shell.execute_reply.started":"2025-02-21T12:54:05.537552Z","shell.execute_reply":"2025-02-21T12:54:36.046061Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output = inference_one_image('./imgs/img4.png', target_pose_path).images[-1]\noutput.resize((1024, 512), Image.BICUBIC)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:54:36.047941Z","iopub.execute_input":"2025-02-21T12:54:36.048185Z","iopub.status.idle":"2025-02-21T12:55:07.370765Z","shell.execute_reply.started":"2025-02-21T12:54:36.048165Z","shell.execute_reply":"2025-02-21T12:55:07.369768Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output = inference_one_image('./imgs/img5.png', './imgs/pose2.png').images[-1]\noutput.resize((512, 512), Image.BICUBIC)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:55:07.371846Z","iopub.execute_input":"2025-02-21T12:55:07.372289Z","iopub.status.idle":"2025-02-21T12:55:38.603037Z","shell.execute_reply.started":"2025-02-21T12:55:07.372250Z","shell.execute_reply":"2025-02-21T12:55:38.602068Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output = inference_one_image('./imgs/img6.png', './imgs/pose3.png').images[-1]\noutput.resize((512, 512), Image.BICUBIC)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:55:38.604074Z","iopub.execute_input":"2025-02-21T12:55:38.604377Z","iopub.status.idle":"2025-02-21T12:56:09.153211Z","shell.execute_reply.started":"2025-02-21T12:55:38.604350Z","shell.execute_reply":"2025-02-21T12:56:09.152294Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output = inference_one_image('./imgs/img7.png', './imgs/pose4.png').images[-1]\noutput.resize((512, 512), Image.BICUBIC)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:56:09.154219Z","iopub.execute_input":"2025-02-21T12:56:09.154524Z","iopub.status.idle":"2025-02-21T12:56:39.640916Z","shell.execute_reply.started":"2025-02-21T12:56:09.154497Z","shell.execute_reply":"2025-02-21T12:56:39.639982Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output = inference_one_image('./imgs/img8.png', './imgs/pose5.png').images[-1]\noutput.resize((512, 512), Image.BICUBIC)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:56:39.641752Z","iopub.execute_input":"2025-02-21T12:56:39.641981Z","iopub.status.idle":"2025-02-21T12:57:10.177289Z","shell.execute_reply.started":"2025-02-21T12:56:39.641962Z","shell.execute_reply":"2025-02-21T12:57:10.176376Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"output = inference_one_image('./imgs/img9.png', './imgs/pose6.png').images[-1]\noutput.resize((512, 512), Image.BICUBIC)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:57:10.178236Z","iopub.execute_input":"2025-02-21T12:57:10.178556Z","iopub.status.idle":"2025-02-21T12:57:40.710533Z","shell.execute_reply.started":"2025-02-21T12:57:10.178526Z","shell.execute_reply":"2025-02-21T12:57:40.709531Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Play with Poses","metadata":{}},{"cell_type":"code","source":"from controlnet_aux import OpenposeDetector\nopenpose = OpenposeDetector.from_pretrained(\"lllyasviel/ControlNet\")\nopenpose(s_img)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:57:40.711506Z","iopub.execute_input":"2025-02-21T12:57:40.711765Z","iopub.status.idle":"2025-02-21T12:57:48.150055Z","shell.execute_reply.started":"2025-02-21T12:57:40.711743Z","shell.execute_reply":"2025-02-21T12:57:48.149211Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"s_pose = inference_pose(s_img_path, image_size=(image_size[1], image_size[0])).resize(image_size, Image.BICUBIC)\nprint('source image width: {}, height: {}'.format(s_pose.width, s_pose.height))\ns_pose","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-21T12:57:48.150909Z","iopub.execute_input":"2025-02-21T12:57:48.151271Z","iopub.status.idle":"2025-02-21T12:57:50.237179Z","shell.execute_reply.started":"2025-02-21T12:57:48.151237Z","shell.execute_reply":"2025-02-21T12:57:50.236375Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}